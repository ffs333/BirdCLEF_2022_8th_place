{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:15:04.153704Z",
     "iopub.status.busy": "2022-05-26T05:15:04.153275Z",
     "iopub.status.idle": "2022-05-26T05:15:26.042471Z",
     "shell.execute_reply": "2022-05-26T05:15:26.041629Z",
     "shell.execute_reply.started": "2022-05-26T05:15:04.153577Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install timm --no-index --find-links=file:///kaggle/input/ast-materials/timm04\n",
    "!pip install audiomentations --no-index --find-links=file:///kaggle/input/packages/audiomentations/pck/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:15:26.044953Z",
     "iopub.status.busy": "2022-05-26T05:15:26.044665Z",
     "iopub.status.idle": "2022-05-26T05:15:30.989442Z",
     "shell.execute_reply": "2022-05-26T05:15:30.988385Z",
     "shell.execute_reply.started": "2022-05-26T05:15:26.044904Z"
    }
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from typing import Optional\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import gc\n",
    "import librosa\n",
    "\n",
    "import scipy\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data as torchdata\n",
    "import torchaudio as ta\n",
    "import soundfile\n",
    "\n",
    "#from torch_audiomentations import Compose, Gain, Shift, PeakNormalization, PitchShift, AddColoredNoise\n",
    "from torchaudio import transforms as T\n",
    "\n",
    "\n",
    "from typing import Callable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "from ast import literal_eval as LE\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.pytorch.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import noisereduce as nr\n",
    "import audiomentations\n",
    "from audiomentations import Normalize as Normalize_aud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:15:30.995697Z",
     "iopub.status.busy": "2022-05-26T05:15:30.995361Z",
     "iopub.status.idle": "2022-05-26T05:15:31.014075Z",
     "shell.execute_reply": "2022-05-26T05:15:31.012889Z",
     "shell.execute_reply.started": "2022-05-26T05:15:30.995655Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#device = get_device()\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:15:31.018588Z",
     "iopub.status.busy": "2022-05-26T05:15:31.018397Z",
     "iopub.status.idle": "2022-05-26T05:15:31.106135Z",
     "shell.execute_reply": "2022-05-26T05:15:31.104985Z",
     "shell.execute_reply.started": "2022-05-26T05:15:31.018564Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    ######################\n",
    "    # Globals #\n",
    "    ######################\n",
    "    seed = 42    \n",
    "    \n",
    "    num_classes = 152\n",
    "    in_channels_ast = 1\n",
    "    target_columns = 'afrsil1 akekee akepa1 akiapo akikik amewig aniani apapan arcter \\\n",
    "                      barpet bcnher belkin1 bkbplo bknsti bkwpet blkfra blknod bongul \\\n",
    "                      brant brnboo brnnod brnowl brtcur bubsan buffle bulpet burpar buwtea \\\n",
    "                      cacgoo1 calqua cangoo canvas caster1 categr chbsan chemun chukar cintea \\\n",
    "                      comgal1 commyn compea comsan comwax coopet crehon dunlin elepai ercfra eurwig \\\n",
    "                      fragul gadwal gamqua glwgul gnwtea golphe grbher3 grefri gresca gryfra gwfgoo \\\n",
    "                      hawama hawcoo hawcre hawgoo hawhaw hawpet1 hoomer houfin houspa hudgod iiwi incter1 \\\n",
    "                      jabwar japqua kalphe kauama laugul layalb lcspet leasan leater1 lessca lesyel lobdow lotjae \\\n",
    "                      madpet magpet1 mallar3 masboo mauala maupar merlin mitpar moudov norcar norhar2 normoc norpin \\\n",
    "                      norsho nutman oahama omao osprey pagplo palila parjae pecsan peflov perfal pibgre pomjae puaioh \\\n",
    "                      reccar redava redjun redpha1 refboo rempar rettro ribgul rinduc rinphe rocpig rorpar rudtur ruff \\\n",
    "                      saffin sander semplo sheowl shtsan skylar snogoo sooshe sooter1 sopsku1 sora spodov sposan \\\n",
    "                      towsol wantat1 warwhe1 wesmea wessan wetshe whfibi whiter whttro wiltur yebcar yefcan zebdov'.split()\n",
    "    \n",
    "    scored = [\"akiapo\", \"aniani\", \"apapan\", \"barpet\", \"crehon\", \"elepai\", \"ercfra\", \"hawama\", \"hawcre\",\n",
    "              \"hawgoo\", \"hawhaw\", \"hawpet1\", \"houfin\", \"iiwi\", \"jabwar\", \"maupar\", \n",
    "              \"omao\", \"puaioh\", \"skylar\", \"warwhe1\", \"yefcan\"]\n",
    "    \n",
    "    rare = ['omao', 'akiapo', 'barpet','hawama', 'elepai', 'aniani', 'hawgoo', 'ercfra', 'maupar',\n",
    "            'hawpet1', 'hawhaw', 'crehon', 'puaioh']\n",
    "\n",
    "    period = 5\n",
    "    n_mels = 224 # 128\n",
    "    n_mels_sed = 224\n",
    "    sample_rate = 32000\n",
    "    \n",
    "    var_mean = -6.98279\n",
    "    var_std = 3.05492\n",
    "    \n",
    "    cuda_num = 0\n",
    "    device = get_device()\n",
    "    \n",
    "    base_model_name = \"tf_efficientnet_b0_ns\"\n",
    "    pooling = \"max\"\n",
    "    pretrained = False\n",
    "    in_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:15:31.113644Z",
     "iopub.status.busy": "2022-05-26T05:15:31.111114Z",
     "iopub.status.idle": "2022-05-26T05:15:31.162062Z",
     "shell.execute_reply": "2022-05-26T05:15:31.161118Z",
     "shell.execute_reply.started": "2022-05-26T05:15:31.113588Z"
    }
   },
   "outputs": [],
   "source": [
    "def interpolate(x: torch.Tensor, ratio: int):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the\n",
    "    resolution reduction in downsampling of a CNN.\n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to interpolate\n",
    "    Returns:\n",
    "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n",
    "    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n",
    "    is the same as the value of the last frame.\n",
    "    Args:\n",
    "      framewise_output: (batch_size, frames_num, classes_num)\n",
    "      frames_num: int, number of frames to pad\n",
    "    Outputs:\n",
    "      output: (batch_size, frames_num, classes_num)\n",
    "    \"\"\"\n",
    "    output = F.interpolate(\n",
    "        framewise_output.unsqueeze(1),\n",
    "        size=(frames_num, framewise_output.size(2)),\n",
    "        align_corners=True,\n",
    "        mode=\"bilinear\").squeeze(1)\n",
    "\n",
    "    return output\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "class Model_SED(nn.Module):\n",
    "    def __init__(self, base_model_name: str, pretrained=False, num_classes=24,\n",
    "                 in_channels=1, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._dropout_rate = dropout_rate\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm2d(CFG.n_mels_sed)\n",
    "\n",
    "        base_model = timm.create_model(\n",
    "            base_model_name, pretrained=pretrained, in_chans=in_channels)\n",
    "        layers = list(base_model.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        if hasattr(base_model, \"fc\"):\n",
    "            in_features = base_model.fc.in_features\n",
    "        else:\n",
    "            in_features = base_model.classifier.in_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "        self.att_block = AttBlockV2(\n",
    "            in_features, num_classes, activation=\"sigmoid\")\n",
    "        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = input_data # (batch_size, 3, time_steps, mel_bins)\n",
    "\n",
    "        frames_num = x.shape[2]\n",
    "\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # Aggregate in frequency axis\n",
    "        x = torch.mean(x, dim=3)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=self._dropout_rate, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=self._dropout_rate, training=self.training)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        interpolate_ratio = frames_num // segmentwise_output.size(1)\n",
    "\n",
    "        # Get framewise output\n",
    "        framewise_output = interpolate(segmentwise_output,\n",
    "                                       interpolate_ratio)\n",
    "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
    "\n",
    "        framewise_logit = interpolate(segmentwise_logit, interpolate_ratio)\n",
    "        framewise_logit = pad_framewise_output(framewise_logit, frames_num)\n",
    "\n",
    "        output_dict = {\n",
    "            'framewise_output': framewise_output,\n",
    "            'clipwise_output': clipwise_output,\n",
    "            'logit': logit,\n",
    "            'framewise_logit': framewise_logit,\n",
    "        }\n",
    "\n",
    "        return output_dict\n",
    "    \n",
    "class Model_linear(nn.Module):\n",
    "    def __init__(self, base_model_name: str, pretrained=False, num_classes=24,\n",
    "                 in_channels=1, dropout_rate=0.4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(base_model_name, pretrained=pretrained, in_chans=in_channels)\n",
    "\n",
    "        self.fc1 = nn.Linear(1000, num_classes)        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.encoder(input_data)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:15:31.171163Z",
     "iopub.status.busy": "2022-05-26T05:15:31.168339Z",
     "iopub.status.idle": "2022-05-26T05:15:31.230753Z",
     "shell.execute_reply": "2022-05-26T05:15:31.229887Z",
     "shell.execute_reply.started": "2022-05-26T05:15:31.171063Z"
    }
   },
   "outputs": [],
   "source": [
    "AUDIO_PATH = '../input/birdclef-2022/train_audio'\n",
    "CLASSES = CFG.target_columns\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "class AudioParams:\n",
    "    \"\"\"\n",
    "    Parameters used for the audio data\n",
    "    \"\"\"\n",
    "    sr = 32000\n",
    "    duration = 5\n",
    "    # Melspectrogram\n",
    "    n_mels = 224\n",
    "    fmin = 20\n",
    "    fmax = 16000\n",
    "    hop_length = 501\n",
    "    n_fft=2048\n",
    "    \n",
    "TARGET_SR = 32000\n",
    "DATADIR = Path(\"../input/birdclef-2022/test_soundscapes/\")\n",
    "datadir2 = Path(\"../input/sndscps10/ana_b\")\n",
    "datadir3 = Path(\"../input/sndscps10/ana_no\")\n",
    "\n",
    "if len(list(DATADIR.glob(\"*.ogg\"))) == 1:\n",
    "    all_audios = list(DATADIR.glob(\"*.ogg\")) + list(datadir2.glob(\"*.wav\")) + list(datadir3.glob(\"*.wav\"))\n",
    "else:\n",
    "    all_audios = list(DATADIR.glob(\"*.ogg\"))\n",
    "\n",
    "sample_submission = pd.read_csv('../input/birdclef-2022/sample_submission.csv')\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:15:31.235570Z",
     "iopub.status.busy": "2022-05-26T05:15:31.235115Z",
     "iopub.status.idle": "2022-05-26T05:15:31.258481Z",
     "shell.execute_reply": "2022-05-26T05:15:31.257686Z",
     "shell.execute_reply.started": "2022-05-26T05:15:31.235525Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_mean_std(df):\n",
    "    mean=[]\n",
    "    std=[]\n",
    "    SR_ = 32000\n",
    "    for i in tqdm(range(len(df))):\n",
    "        clip, sr = ta.load(df[i])\n",
    "        if clip.size(0) != 1:\n",
    "            clip = clip.mean(dim=0, keepdim=True)\n",
    "            \n",
    "        if sr != SR_:\n",
    "            clip = taF.resample(clip, sr, SR_, lowpass_filter_width=64,\n",
    "                                            rolloff=0.9475937167399596, resampling_method=\"kaiser_window\",\n",
    "                                            beta=14.769656459379492)\n",
    "        \n",
    "        audio_input = clip - clip.mean()\n",
    "        audio_input = ta.compliance.kaldi.fbank(audio_input, htk_compat=True, sample_frequency=SR_, use_energy=False,\n",
    "                                                  window_type='hanning', num_mel_bins=224, \n",
    "                                                  dither=0.0, frame_shift=9.7)\n",
    "        \n",
    "        cur_mean = torch.mean(audio_input)\n",
    "        cur_std = torch.std(audio_input)\n",
    "        mean.append(cur_mean)\n",
    "        std.append(cur_std)\n",
    "        #print(cur_mean, cur_std)\n",
    "    return np.mean(mean), np.mean(std)\n",
    "\n",
    "#mean, std = get_mean_std(train_data)\n",
    "\n",
    "GENERATE_NEW_MEAN_STD = False\n",
    "if GENERATE_NEW_MEAN_STD:\n",
    "    tar_mean, tar_std = get_mean_std(all_audios)\n",
    "else:\n",
    "    tar_mean, tar_std = CFG.var_mean, CFG.var_std\n",
    "    \n",
    "print(tar_mean, tar_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:15:31.265407Z",
     "iopub.status.busy": "2022-05-26T05:15:31.263222Z",
     "iopub.status.idle": "2022-05-26T05:15:31.287998Z",
     "shell.execute_reply": "2022-05-26T05:15:31.287085Z",
     "shell.execute_reply.started": "2022-05-26T05:15:31.265371Z"
    }
   },
   "outputs": [],
   "source": [
    "val_audio_conf = {'num_mel_bins': 224, 'target_length': 512, 'freqm': 0, 'timem': 0, \n",
    "              'mixup': -1, 'skip_norm': True, 'mode': 'eval', 'dataset': 'Birds custom',\n",
    "             'mean':tar_mean, 'std': tar_std, 'noise': False}\n",
    "\n",
    "\n",
    "def crop_or_pad(y, sr=CFG.sample_rate, length=CFG.sample_rate*CFG.period, mode='eval'):\n",
    "    leny = y.size(1)\n",
    "    if leny <= length:\n",
    "        y = torch.nn.functional.pad(y, (0,length - leny), \"constant\", 0)\n",
    "    else:\n",
    "        if mode != 'train':\n",
    "            start_ = 0\n",
    "        else:        \n",
    "            start_ = np.random.randint(leny - length)\n",
    "        y = y[:, start_: start_ + length]\n",
    "    return y\n",
    "\n",
    "def compute_melspec(y, params, to_db=True):\n",
    "    \"\"\"\n",
    "    Computes a mel-spectrogram and puts it at decibel scale\n",
    "    Arguments:\n",
    "        y {np array} -- signal\n",
    "        params {AudioParams} -- Parameters to use for the spectrogram. Expected to have the attributes sr, \n",
    "        n_mels, f_min, f_max\n",
    "    Returns:\n",
    "        np array -- Mel-spectrogram\n",
    "    \"\"\"\n",
    "    melspec = librosa.feature.melspectrogram(\n",
    "        y=y, sr=params.sr, n_mels=params.n_mels, fmin=params.fmin, fmax=params.fmax,\n",
    "        hop_length=params.hop_length, n_fft=params.n_fft)  #**2.5 #####\n",
    "    if to_db:\n",
    "        melspec = librosa.power_to_db(melspec).astype(np.float32)\n",
    "    return melspec\n",
    "\n",
    "def mono_to_color(X, eps=1e-6, mean=None, std=None):\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    X = (X - mean) / (std + eps)\n",
    "\n",
    "    # Normalize to [0, 255]\n",
    "    _min, _max = X.min(), X.max()\n",
    "\n",
    "    if (_max - _min) > eps:\n",
    "        V = np.clip(X, _min, _max)\n",
    "        V = 255 * (V - _min) / (_max - _min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        V = np.zeros_like(X, dtype=np.uint8)\n",
    "\n",
    "    return V\n",
    "\n",
    "mean = (0.485, 0.456, 0.406) # RGB\n",
    "std = (0.229, 0.224, 0.225) # RGB\n",
    "\n",
    "albu_transforms = {'valid' : A.Compose([A.Normalize(mean, std)])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:15:31.296633Z",
     "iopub.status.busy": "2022-05-26T05:15:31.293447Z",
     "iopub.status.idle": "2022-05-26T05:15:31.325960Z",
     "shell.execute_reply": "2022-05-26T05:15:31.325015Z",
     "shell.execute_reply.started": "2022-05-26T05:15:31.296594Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class AudiosetDataset(Dataset):\n",
    "    def __init__(self, df_in: pd.DataFrame, clip_ast, audio_conf=val_audio_conf, label_csv=None):\n",
    "        \"\"\"\n",
    "        Dataset that manages audio recordings\n",
    "        :param audio_conf: Dictionary containing the audio loading and preprocessing settings\n",
    "        :param dataset_json_file\n",
    "        \"\"\"\n",
    "        self.data = df_in\n",
    "        self.clip_ast = clip_ast\n",
    "\n",
    "        self.audio_conf = audio_conf\n",
    "        \n",
    "        self.melbins = self.audio_conf.get('num_mel_bins')    \n",
    "        \n",
    "        self.dataset = self.audio_conf.get('dataset')\n",
    "        # dataset spectrogram mean and std, used to normalize the input\n",
    "        self.norm_mean = self.audio_conf.get('mean')\n",
    "        self.norm_std = self.audio_conf.get('std')\n",
    "        # if add noise for data augmentation\n",
    "        self.noise = self.audio_conf.get('noise')\n",
    "        self.label_num = len(CFG.target_columns)\n",
    "        \n",
    "        self.SR_ = 32000\n",
    "        self.wt = audiomentations.Compose([Normalize_aud(p=1)])\n",
    "        \n",
    "    def _wav2fbank(self, waveform):\n",
    "            \n",
    "        #waveform, sr = ta.load(filename)\n",
    "        waveform = crop_or_pad(waveform, sr=self.SR_, mode='eval')\n",
    "        waveform = waveform - waveform.mean()\n",
    "        waveform = torch.tensor(self.wt(samples=waveform[0].numpy(), sample_rate=self.SR_)).unsqueeze(0)\n",
    "        fbank = ta.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=self.SR_, \n",
    "                                                  use_energy=False, window_type='hanning', \n",
    "                                                  num_mel_bins=self.melbins, dither=0.0, frame_shift=9.7)\n",
    "        #fbank = fbank ** 2.5 #########\n",
    "        target_length = self.audio_conf.get('target_length')\n",
    "        n_frames = fbank.shape[0]\n",
    "        p = target_length - n_frames\n",
    "        # cut and pad\n",
    "        if p > 0:\n",
    "            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "            fbank = m(fbank)\n",
    "        elif p < 0:\n",
    "            fbank = fbank[0:target_length, :]\n",
    "        return fbank\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        returns: image, audio, nframes\n",
    "        where image is a FloatTensor of size (3, H, W)\n",
    "        audio is a FloatTensor of size (N_freq, N_frames) for spectrogram, or (N_frames) for waveform\n",
    "        nframes is an integer\n",
    "        \"\"\"\n",
    "        SR = 32000\n",
    "        sample = self.data.loc[index, :]\n",
    "        row_id = sample.row_id\n",
    "        \n",
    "        end_seconds = int(sample.seconds)\n",
    "        start_seconds = int(end_seconds - 5)\n",
    "        \n",
    "        # ast preproc\n",
    "        img_ast = self.clip_ast[:, SR*start_seconds:SR*end_seconds]         \n",
    "        \n",
    "        fbank = self._wav2fbank(img_ast) \n",
    "        #fbank = torch.transpose(fbank, 0, 1)\n",
    "        fbank = (fbank - self.norm_mean) / (self.norm_std * 2)\n",
    "\n",
    "        fbank = torch.stack([fbank, fbank, fbank])\n",
    "        \n",
    "        \n",
    "        image = img_ast[0].numpy()\n",
    "        image = self.wt(samples=image, sample_rate=SR)\n",
    "        image = np.nan_to_num(image)\n",
    "        \n",
    "        image = compute_melspec(image, AudioParams, to_db=True)  \n",
    "        image = mono_to_color(image)\n",
    "        \n",
    "        image = image.astype(np.uint8)\n",
    "        image = albu_transforms['valid'](image=image)['image'].T\n",
    "\n",
    "        # the output fbank shape is [time_frame_num, frequency_bins], e.g., [1024, 128]\n",
    "        return {\n",
    "            \"image_ast\": fbank,\n",
    "            \"image_sed\": image,\n",
    "            \"row_id\": row_id,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:15:31.336647Z",
     "iopub.status.busy": "2022-05-26T05:15:31.333567Z",
     "iopub.status.idle": "2022-05-26T05:15:44.758943Z",
     "shell.execute_reply": "2022-05-26T05:15:44.758196Z",
     "shell.execute_reply.started": "2022-05-26T05:15:31.336606Z"
    }
   },
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "path_image_base = '../input/checks-triple/_att_6_BEST_FOR_IMAGE.ckpt'\n",
    "\n",
    "path_5_2_6 = '../input/checks-triple/_att_5_2_trans_new_data_811.ckpt'  \n",
    "\n",
    "path_6_1_3 = '../input/checks-triple/_att_6_1_03secondary_0845.ckpt'\n",
    "\n",
    "path_pack1 = [path_image_base, path_5_2_6, path_6_1_3]\n",
    "######################################################################################################\n",
    "\n",
    "models_pack1 = []\n",
    "for ppath in path_pack1:\n",
    "    \n",
    "    model_image_base =  Model_SED(\n",
    "                                base_model_name=CFG.base_model_name,\n",
    "                                pretrained=CFG.pretrained,\n",
    "                                num_classes=CFG.num_classes,\n",
    "                                in_channels=CFG.in_channels)\n",
    "\n",
    "    model_image_base.to(CFG.device)\n",
    "    model_image_base.load_state_dict(torch.load(ppath, map_location=CFG.device))  # path_image_base\n",
    "    model_image_base.eval()\n",
    "    models_pack1.append(('sed', model_image_base))\n",
    "\n",
    "print(f'Length of model pack 1: {len(models_pack1)} ')\n",
    "\n",
    "######################################################################################################\n",
    "path_9_f0 = '../input/checks-triple/app9/_9_att_1_best05_fold0.ckpt'\n",
    "path_9_f1 = '../input/checks-triple/app9/_9_att_1_best05_fold1.ckpt'\n",
    "path_9_f2 = '../input/checks-triple/app9/_9_att_1_best05_fold2.ckpt'\n",
    "path_9_f3 = '../input/checks-triple/app9/_9_att_2_best05_fold3.ckpt'\n",
    "path_9_f4 = '../input/checks-triple/app9/_9_att_2_best05_fold4.ckpt'\n",
    "path_9_f5 = '../input/checks-triple/app9/_9_att_3_best05_fold5.ckpt'\n",
    "path_9_f6 = '../input/checks-triple/app9/_9_att_3_best05_fold6.ckpt'\n",
    "\n",
    "path_pack2 = [path_9_f0, path_9_f1, path_9_f2, path_9_f3, path_9_f4, path_9_f5, path_9_f6]\n",
    "\n",
    "######################################################################################################\n",
    "models_pack2 = []\n",
    "for ppath in path_pack2:\n",
    "    model_9_f0 =  Model_SED(\n",
    "                                base_model_name=CFG.base_model_name,\n",
    "                                pretrained=CFG.pretrained,\n",
    "                                num_classes=CFG.num_classes,\n",
    "                                in_channels=CFG.in_channels)\n",
    "\n",
    "    model_9_f0.to(CFG.device)\n",
    "    model_9_f0.load_state_dict(torch.load(ppath, map_location=CFG.device))  # path_image_base\n",
    "    model_9_f0.eval()\n",
    "    models_pack2.append(('sed', model_9_f0))\n",
    "\n",
    "print(f'Length of model pack 2: {len(models_pack2)} ')\n",
    "\n",
    "\n",
    "######################################################################################################\n",
    "path_f0 = '../input/checks-triple/_att_6_1_03secondary_0858.ckpt'\n",
    "path_f1 = '../input/checks-triple/_att_6_2_05secondary_0850.ckpt'\n",
    "path_f2 = '../input/checks-triple/app8/_att_4_best05_fold0_.ckpt'\n",
    "path_f3 = '../input/checks-triple/app8/_att_7_best_fold0.ckpt'\n",
    "path_f4 = '../input/checks-triple/app8/_att_1_best05_fold1_.ckpt'\n",
    "\n",
    "path_pack3 = [path_f0, path_f1, path_f2, path_f4] #path_f3, path_f4]\n",
    "\n",
    "######################################################################################################\n",
    "models_pack3 = []\n",
    "for ppath in path_pack3:\n",
    "    model_9_f0 =  Model_SED(\n",
    "                                base_model_name=CFG.base_model_name,\n",
    "                                pretrained=CFG.pretrained,\n",
    "                                num_classes=CFG.num_classes,\n",
    "                                in_channels=CFG.in_channels)\n",
    "\n",
    "    model_9_f0.to(CFG.device)\n",
    "    model_9_f0.load_state_dict(torch.load(ppath, map_location=CFG.device))  # path_image_base\n",
    "    model_9_f0.eval()\n",
    "    models_pack3.append(('sed', model_9_f0))\n",
    "\n",
    "print(f'Length of model pack 3: {len(models_pack3)} ')\n",
    "\n",
    "# SEDMIX\n",
    "path_SEDMIX_1 = '../input/ftw-new/CHECKS_SEDMIX/SEDMIX_f02_last_epoch.bin'\n",
    "path_SEDMIX_2 = '../input/ftw-new/CHECKS_SEDMIX/SEDMIX_f02_best_03_epoch_train830_eval767.bin'\n",
    "path_SEDMIX_3 = '../input/ftw-new/CHECKS_SEDMIX/SEDMIX_f02_best_03_epoch_train833_eval766.bin'\n",
    "\n",
    "path_SEDMIXFULL_pret_1 = '../input/ftw-new/CHECKS_SEDMIX_FULL/SEDMIXFULL_fromcheck_TARGET_03BEST_881_981_808.bin'\n",
    "path_SEDMIXFULL_pret_2 = '../input/ftw-new/CHECKS_SEDMIX_FULL/SEDMIXFULL_fromcheck_TARGET_03BEST_881_987_809.bin'\n",
    "path_SEDMIXFULL_pret_3 = '../input/ftw-new/CHECKS_SEDMIX_FULL/SEDMIXFULL_fromcheck_TRAIN_03BEST_902_985_820.bin'\n",
    "\n",
    "path_SEDMIXFULL_1 = '../input/ftw-new/CHECKS_SEDMIX_FULL/SEDMIXFULL_VALID_03BEST_839_965_770.bin'\n",
    "path_SEDMIXFULL_2 = '../input/ftw-new/CHECKS_SEDMIX_FULL/SEDMIXFULL_TRAIN_03BEST_870_968_801.bin'\n",
    "\n",
    "path_pack4 = [path_SEDMIX_2, path_SEDMIXFULL_pret_3, path_SEDMIXFULL_2] \n",
    "models_pack4 = []\n",
    "for ppath in path_pack4:     \n",
    "    model_image_base =  Model_SED(\n",
    "                                base_model_name=CFG.base_model_name,\n",
    "                                pretrained=CFG.pretrained,\n",
    "                                num_classes=CFG.num_classes,\n",
    "                                in_channels=CFG.in_channels)\n",
    "\n",
    "    model_image_base.to(CFG.device)\n",
    "    model_image_base.load_state_dict(torch.load(ppath, map_location=CFG.device))  # path_image_base\n",
    "    model_image_base.eval()\n",
    "    models_pack4.append(('ast', model_image_base))\n",
    "\n",
    "print(f'Length of model pack 4: {len(models_pack4)} ')\n",
    "\n",
    "\n",
    "path_pseudo_1 = '../input/ftw-new/pseudo/PSEUDO_last_epoch_SAVE_974_971_893.bin'\n",
    "path_pseudo_2 = '../input/ftw-new/pseudo/PSEUDO_TARGET_03BEST_GO.bin'\n",
    "path_pseudo_3 = '../input/ftw-new/pseudo/PSEUDO_last_epoch.bin'\n",
    "\n",
    "\n",
    "path_pack5 = [path_pseudo_1, path_pseudo_2, path_pseudo_3] \n",
    "models_pack5 = []\n",
    "for ppath in path_pack5:     \n",
    "    model_image_base =  Model_SED(\n",
    "                                base_model_name=CFG.base_model_name,\n",
    "                                pretrained=CFG.pretrained,\n",
    "                                num_classes=CFG.num_classes,\n",
    "                                in_channels=CFG.in_channels)\n",
    "\n",
    "    model_image_base.to(CFG.device)\n",
    "    model_image_base.load_state_dict(torch.load(ppath, map_location=CFG.device))  # path_image_base\n",
    "    model_image_base.eval()\n",
    "    models_pack5.append(('ast', model_image_base))\n",
    "\n",
    "print(f'Length of model pack 5: {len(models_pack5)} ')\n",
    "\n",
    "\n",
    "\n",
    "# NOCALL CLS\n",
    "\n",
    "path_NOCALL_1 = '../input/ftw-new/NOCALLCLS_f00_last_epoch.bin'\n",
    "path_NOCALL_2 = '../input/ftw-new/NOCALLCLS_f00_best_930.bin'\n",
    "path_NOCALL_3 = '../input/ftw-new/NOCALLCLS_f00_best_936.bin'\n",
    "\n",
    "model_cls = Model_linear(base_model_name=CFG.base_model_name,\n",
    "                        pretrained=False,\n",
    "                        num_classes=2,\n",
    "                        in_channels=CFG.in_channels)\n",
    "\n",
    "model_cls.to(CFG.device)\n",
    "model_cls.load_state_dict(torch.load(path_NOCALL_3, map_location=CFG.device))\n",
    "model_cls.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:15:44.760653Z",
     "iopub.status.busy": "2022-05-26T05:15:44.760393Z",
     "iopub.status.idle": "2022-05-26T05:15:44.778902Z",
     "shell.execute_reply": "2022-05-26T05:15:44.778094Z",
     "shell.execute_reply.started": "2022-05-26T05:15:44.760617Z"
    }
   },
   "outputs": [],
   "source": [
    "def prediction_for_clip(test_df, \n",
    "                        clip_ast,\n",
    "                        device,\n",
    "                        models, model_cls,\n",
    "                        threshold,\n",
    "                        weights,\n",
    "                        wgt_models,\n",
    "                        wg_full):\n",
    "    BS = 12 #isinstance(model_pack[3][0], ASTModel)\n",
    "    dataset = AudiosetDataset(test_df, clip_ast)\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=BS, shuffle=False)\n",
    "    prediction_dict = {}\n",
    "    for data in loader:\n",
    "        row_id = data['row_id']\n",
    "        image_ast = data['image_ast'].to(device)        \n",
    "        image_sed = data['image_sed'].to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            full_events_all = []\n",
    "            #nocl = model_cls(image_ast.transpose(2,3)).softmax(1).cpu().numpy()\n",
    "            #nocl = np.array([float(fr[0] < 0.83) for fr in nocl])\n",
    "            #print(row_id[0], nocl)\n",
    "\n",
    "            for md in range(len(models)):\n",
    "                \n",
    "                full_events = []\n",
    "                for i in range(len(models[md])):\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if models[md][i][0] == 'ast':\n",
    "                            output = models[md][i][1](image_ast)\n",
    "                        else:\n",
    "                            output = models[md][i][1](image_sed)\n",
    "                        probs = output['clipwise_output'].detach().cpu().numpy() \n",
    "                        #probs = torch.sigmoid(output['logit']).detach().cpu().numpy() \n",
    "\n",
    "                        probs = probs * weights[md][i] #* wg_full[md]\n",
    "                        #probs = probs * nocl.reshape(-1, 1)\n",
    "                    full_events.append(probs)\n",
    "                    \n",
    "                full_events = np.array(full_events).transpose(1, 0, 2)\n",
    "                \n",
    "                ### POST PROCESSING\n",
    "                USE_MEAN_MEDIAN = True\n",
    "                USE_MAX_ADDER = True\n",
    "\n",
    "                if USE_MEAN_MEDIAN:\n",
    "                    full_med = np.median(full_events, axis=1) \n",
    "                    full_mean = np.mean(full_events, axis=1)\n",
    "                    full_events = np.mean(np.array([full_med, full_mean]), axis=0)                \n",
    "                else:        \n",
    "                    full_events = full_events * wg_full.reshape(-1, 1) \n",
    "                    full_events = np.sum(full_events, axis=1)  \n",
    "                if USE_MAX_ADDER:\n",
    "                    logits_max = full_events.max(0)\n",
    "                    for jk in range(full_events.shape[1]):\n",
    "                        if logits_max[jk] > threshold * 2.5:\n",
    "                            full_events[:, jk] += threshold * 0.5\n",
    "                            \n",
    "                full_events_all.append(full_events)\n",
    "                \n",
    "        full_events_all = np.array(full_events_all).transpose(1, 0, 2)\n",
    "        full_events_all = np.mean(full_events_all, axis=1)        \n",
    "        \n",
    "        full_events_all = full_events_all >= threshold\n",
    "        \n",
    "        for kk in range(full_events_all.shape[0]):\n",
    "            labels = np.argwhere(full_events_all[kk]).reshape(-1).tolist()\n",
    "\n",
    "            labels_str_list = list(map(lambda x: CFG.target_columns[x], labels))\n",
    "            #labels_str_list = [x for x in labels_str_list if x in CFG.scored]\n",
    "            label_string = \" \".join(labels_str_list)\n",
    "            if len(label_string) == 0:\n",
    "                label_string = \"nocall\"\n",
    "            prediction_dict[str(row_id[kk])] = label_string\n",
    "    return prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:15:44.781086Z",
     "iopub.status.busy": "2022-05-26T05:15:44.780416Z",
     "iopub.status.idle": "2022-05-26T05:15:44.792532Z",
     "shell.execute_reply": "2022-05-26T05:15:44.791655Z",
     "shell.execute_reply.started": "2022-05-26T05:15:44.781046Z"
    }
   },
   "outputs": [],
   "source": [
    "def prediction(test_audios,\n",
    "               models, model_cls,\n",
    "               threshold,\n",
    "               weights,\n",
    "               wgt_models, \n",
    "               wg_full):\n",
    "    \n",
    "    # models = [model]\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    device = get_device()\n",
    "    prediction_dicts = {}\n",
    "    SR_ = 32000\n",
    "    sec_60_len = SR_ * 60\n",
    "    for audio_path in test_audios:            \n",
    "        clip_ast, sr = ta.load(audio_path)\n",
    "\n",
    "        if clip_ast.size(0) != 1:\n",
    "            clip_ast = clip_ast.mean(dim=0, keepdim=True)\n",
    "            \n",
    "        if clip_ast.size(1) < sec_60_len:\n",
    "            clip_ast = crop_or_pad(clip_ast, SR_, sec_60_len)\n",
    "        if sr != SR_:\n",
    "            clip_ast = taF.resample(clip_ast, sr, SR_, lowpass_filter_width=64,\n",
    "                                            rolloff=0.9475937167399596, resampling_method=\"kaiser_window\",\n",
    "                                            beta=14.769656459379492)\n",
    "        \n",
    "        seconds = []\n",
    "        row_ids = []\n",
    "        for second in range(5, 65, 5):\n",
    "            row_id = \"_\".join(audio_path.name.split(\".\")[:-1]) + f\"_{second}\"\n",
    "            seconds.append(second)\n",
    "            row_ids.append(row_id)\n",
    "        #print(row_ids)\n",
    "        test_df = pd.DataFrame({\n",
    "            \"row_id\": row_ids,\n",
    "            \"seconds\": seconds\n",
    "        })\n",
    "        \n",
    "        prediction_dict = prediction_for_clip(test_df,\n",
    "                                                clip_ast=clip_ast,  \n",
    "                                                device=device,\n",
    "                                                models=models,model_cls=model_cls,\n",
    "                                                threshold=threshold,\n",
    "                                                weights=weights,\n",
    "                                                wgt_models=wgt_models,\n",
    "                                                wg_full=wg_full)\n",
    "\n",
    "        prediction_dicts.update(prediction_dict)\n",
    "    return prediction_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:15:44.794291Z",
     "iopub.status.busy": "2022-05-26T05:15:44.793955Z",
     "iopub.status.idle": "2022-05-26T05:15:44.840964Z",
     "shell.execute_reply": "2022-05-26T05:15:44.840285Z",
     "shell.execute_reply.started": "2022-05-26T05:15:44.794257Z"
    }
   },
   "outputs": [],
   "source": [
    "weights_p1_base_b = np.load('../input/checks-triple/weights_pu.npy')\n",
    "weights_p1_2_b = np.load('../input/checks-triple/weights_ar_250_12.npy')\n",
    "weights_p1_3_b = np.load('../input/checks-triple/weights_ar_300_10.npy')\n",
    "### FOR PACK 1\n",
    "#####################################################################################\n",
    "\n",
    "weights_p2 = np.load('../input/checks-triple/app8/weights_ar_WAVE_800_7.npy')\n",
    "### FOR PACK 2\n",
    "#####################################################################################\n",
    "\n",
    "weights_p3_1 = np.load('../input/checks-triple/weights_ar_300_10.npy')\n",
    "weights_p3_2 = np.load('../input/checks-triple/app8/weights_ar_WAVE_420_7.npy')\n",
    "weights_p3_3 = np.load('../input/checks-triple/app8/weights_ar_WAVE_800_7.npy')\n",
    "weights_p3_4 = np.load('../input/checks-triple/app8/weights_ar_WAVE_250_7.npy')\n",
    "### FOR PACK 3\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:15:44.842570Z",
     "iopub.status.busy": "2022-05-26T05:15:44.842319Z",
     "iopub.status.idle": "2022-05-26T05:15:44.848377Z",
     "shell.execute_reply": "2022-05-26T05:15:44.847450Z",
     "shell.execute_reply.started": "2022-05-26T05:15:44.842535Z"
    }
   },
   "outputs": [],
   "source": [
    "st_p1 = 1. # 0.5 \n",
    "weights_ar_p1 = [weights_p1_base_b**st_p1, weights_p1_2_b**st_p1, weights_p1_3_b**st_p1]\n",
    "#wgt_models_p1 = np.array([0.19, 0.28, 0.53])\n",
    "### FOR PACK 1\n",
    "#####################################################################################\n",
    "\n",
    "st_p2 = 0.35 # 0.35\n",
    "weights_ar_p2 = [weights_p2**st_p2, weights_p2**st_p2, weights_p2**st_p2, weights_p2**st_p2,\n",
    "                 weights_p2**st_p2, weights_p2**st_p2, weights_p2**st_p2]\n",
    "#wgt_models_p2 = np.array([0.15, 0.14, 0.14, 0.14, 0.15, 0.14, 0.14])\n",
    "### FOR PACK 2\n",
    "#####################################################################################\n",
    "\n",
    "st_p3 = 1. #0.5\n",
    "#weights_ar_p3 = [weights_p3_1**st_p3, weights_p3_1**st_p3, weights_p3_2**st_p3, weights_p3_2**st_p3, weights_p3_2**st_p3]\n",
    "weights_ar_p3 = [weights_p3_1**st_p3, weights_p3_1**st_p3, weights_p3_2**st_p3, weights_p3_2**st_p3]\n",
    "#wgt_models_p3 = np.array([0.17, 0.17, 0.2, 0.26, 0.2])\n",
    "### FOR PACK 3\n",
    "#####################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:15:44.850216Z",
     "iopub.status.busy": "2022-05-26T05:15:44.849688Z",
     "iopub.status.idle": "2022-05-26T05:15:44.873658Z",
     "shell.execute_reply": "2022-05-26T05:15:44.873038Z",
     "shell.execute_reply.started": "2022-05-26T05:15:44.850181Z"
    }
   },
   "outputs": [],
   "source": [
    "### FOR PACK 4 SEDMIX\n",
    "#####################################################################################\n",
    "weights_p4 = np.load('../input/ftw-new/CHECKS_SEDMIX/SEDMIX_weights_ar_ast_300_15.npy')\n",
    "floor = 8\n",
    "weights_p4 = np.where(weights_p4 > floor, floor, weights_p4)\n",
    "\n",
    "weights_p4_2 = np.load('../input/ftw-new/CHECKS_SEDMIX_FULL/SEDMIX_NOFOLDS_weights_ar_ast_300_15.npy')\n",
    "floor = 8\n",
    "weights_p4_2 = np.where(weights_p4_2 > floor, floor, weights_p4_2)\n",
    "\n",
    "st_p4 = 0.6  #0.6\n",
    "weights_ar_p4 = [weights_p4**st_p4, weights_p4_2**st_p4, weights_p4_2**st_p4]\n",
    "#wgt_models_p4 = np.array([0.32, 0.35, 0.33])\n",
    "### FOR PACK 4  SEDMIX\n",
    "#####################################################################################\n",
    "\n",
    "\n",
    "\n",
    "weights_p5 = np.load('../input/ftw-new/pseudo/PSEUDO_WGTS_300_15.npy')\n",
    "floor = 8\n",
    "weights_p5 = np.where(weights_p5 > floor, floor, weights_p5)\n",
    "\n",
    "st_p5 = 0.6\n",
    "weights_ar_p5 = [(weights_p5**st_p5), (weights_p5**st_p5), (weights_p5**st_p5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:15:44.875225Z",
     "iopub.status.busy": "2022-05-26T05:15:44.874963Z",
     "iopub.status.idle": "2022-05-26T05:15:44.881842Z",
     "shell.execute_reply": "2022-05-26T05:15:44.881120Z",
     "shell.execute_reply.started": "2022-05-26T05:15:44.875192Z"
    }
   },
   "outputs": [],
   "source": [
    "print_it = False\n",
    "if print_it:\n",
    "\n",
    "    for kk in zip (CFG.target_columns,np.around(weights_p4**st_p4, decimals=2),\n",
    "                                      np.around(weights_p5**st_p5/8*0, decimals=2)):\n",
    "        if kk[0] in CFG.scored:\n",
    "            print(kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:15:44.883602Z",
     "iopub.status.busy": "2022-05-26T05:15:44.883317Z",
     "iopub.status.idle": "2022-05-26T05:15:44.893583Z",
     "shell.execute_reply": "2022-05-26T05:15:44.892651Z",
     "shell.execute_reply.started": "2022-05-26T05:15:44.883566Z"
    }
   },
   "outputs": [],
   "source": [
    "#model_pack = models_pack2 + models_pack4 + models_pack5\n",
    "#weights_pack = weights_ar_p2 + weights_ar_p4 + weights_ar_p5\n",
    "\n",
    "model_pack = [models_pack2 + models_pack4, models_pack5]\n",
    "weights_pack = [weights_ar_p2 + weights_ar_p4, weights_ar_p5]\n",
    "\n",
    "print(len(model_pack), len(weights_pack))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:15:44.895331Z",
     "iopub.status.busy": "2022-05-26T05:15:44.894878Z",
     "iopub.status.idle": "2022-05-26T05:16:16.356974Z",
     "shell.execute_reply": "2022-05-26T05:16:16.356201Z",
     "shell.execute_reply.started": "2022-05-26T05:15:44.895295Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.0795 \n",
    "\n",
    "import time\n",
    "sddt = time.time()\n",
    "\n",
    "prediction_dicts = prediction(test_audios=all_audios,\n",
    "                               models=model_pack, model_cls=model_cls,\n",
    "                               threshold=threshold, \n",
    "                               weights=weights_pack,\n",
    "                               wgt_models=None,\n",
    "                               wg_full=None)\n",
    "\n",
    "for i in range(len(sample_submission)):\n",
    "    sample = sample_submission.row_id[i]\n",
    "    key = sample.split(\"_\")[0] + \"_\" + sample.split(\"_\")[1] + \"_\" + sample.split(\"_\")[3]\n",
    "    target_bird = sample.split(\"_\")[2]\n",
    "    #print(key, target_bird)\n",
    "    if key in prediction_dicts:\n",
    "        sample_submission.iat[i, 1] = (target_bird in prediction_dicts[key])\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(f'Elapsed time: {time.time() - sddt:.3f} seconds')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:16:16.358729Z",
     "iopub.status.busy": "2022-05-26T05:16:16.358317Z",
     "iopub.status.idle": "2022-05-26T05:16:16.469551Z",
     "shell.execute_reply": "2022-05-26T05:16:16.468828Z",
     "shell.execute_reply.started": "2022-05-26T05:16:16.358691Z"
    }
   },
   "outputs": [],
   "source": [
    "#prediction_dicts\n",
    "if len(list(DATADIR.glob(\"*.ogg\"))) == 1:\n",
    "    cntr = 0    \n",
    "    for k in prediction_dicts:\n",
    "        print(f'{k}: ',[x for x in prediction_dicts[k].split(' ') if x in CFG.scored])\n",
    "        cntr += 1\n",
    "        if cntr % 12 == 0:\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
